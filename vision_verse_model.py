# -*- coding: utf-8 -*-
"""vision_verse_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ib5_k3fYvc_ufu2fDBZbWcdJK3ImFNnA
"""

!pip install opendatasets --upgrade

import opendatasets as od
import os
import cv2
import random
import matplotlib.pyplot as plt
import pickle
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img

od.download('https://www.kaggle.com/competitions/vision-verse/data')

train_data_dir = '/content/vision-verse/data/train';
Categories = ['crownandrootrot','healthywheat','leafrust','wheatloosesmut']

# import imageio as iio
# count=0
# for c in Categories:
#   dirr=train_data_dir+"/"+c
#   for l in os.listdir(dirr):
#     imgg=iio.imread(dirr+"/"+l)
#     if l[-1]=='f' and l[-2]=='i' and l[-3]=='g':
#       print(l)

# import imageio as iio
# count=0

# for c in Categories:
#   dirr=train_data_dir+"/"+c
#   for l in os.listdir(dirr):
#     imgg=iio.imread(dirr+"/"+l)
#     imgg=np.resize(imgg, (32, 32, 3))
#     data.append([imgg,Categories.index(c)])

# data=np.array(data)

IMG_SIZE = 224
data=[]
i=0
for category in Categories:
  folder = os.path.join(train_data_dir,category)
  label = Categories.index(category)
  for img in os.listdir(folder):
    img_path = os.path.join(folder,img)
    img_arr = cv2.imread(img_path)
   # print(img_arr)
    # try:
    #        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))
    #        print(img.shape)
    # except:
    #     print('err')
    # print(category,i)
    try:
        # image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH), interpolation=cv2.INTER_AREA)
        img_arr = cv2.resize(img_arr,(IMG_SIZE,IMG_SIZE))
        # print(img_arr.shape)
    except:
        print(img_arr,2)
        continue
   # print(img_path)
    img_arr = cv2.resize(img_arr,(IMG_SIZE,IMG_SIZE))
    data.append([img_arr,label])
    i=i+1

random.shuffle(data)
batch_size = 32

train_datagen = ImageDataGenerator(rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2) # set validation split

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode='binary',
    shuffle = True,
    subset='training') # set as training data

validation_generator = train_datagen.flow_from_directory(
    train_data_dir, # same directory as training data
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode='binary',
    shuffle = True,
    subset='validation') # set as validation data

train_generator

feature = []
label = []

for features,labels in data:
  feature.append(features)
  label.append(labels)
feature = np.array(feature)
label = np.array(label)
feature = feature.astype('float')
feature=feature/255

feature.shape

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

import tensorflow as tf


model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu',input_shape=(224, 224, 3)),
    tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),
    tf.keras.layers.MaxPool2D(pool_size=(2,2)),
    tf.keras.layers.BatchNormalization(axis=-1),

    tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),
    tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'),
    tf.keras.layers.MaxPool2D(pool_size=(2,2)),
    tf.keras.layers.BatchNormalization(axis=-1),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512,activation='relu'),
    tf.keras.layers.BatchNormalization() ,
    tf.keras.layers.Dropout(rate=0.5),

    tf.keras.layers.Dense(4,activation='softmax')

])
learning_rate = 0.001
epochs=2
opt= tf.keras.optimizers.Adam(learning_rate=learning_rate , decay=learning_rate/(epochs*0.5))
model.compile(optimizer='adam',loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])

model.fit(feature, label, epochs=10, validation_split=0.1)

dir = '/content/vision-verse/data/test'
df = pd.read_csv('/content/vision-verse/data/sample_submission.csv')
print(df)

testing = []
c=0
count =0
for img in os.listdir(dir):
    img_path = os.path.join(dir,img)
    img_arr = cv2.imread(img_path)
    try:
        # image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH), interpolation=cv2.INTER_AREA)
        img_arr = cv2.resize(img_arr,(IMG_SIZE,IMG_SIZE))
        # img_arr = cv2.resize(img_arr,(IMG_SIZE,IMG_SIZE))
        # 'crownandrootrot','healthywheat','leafrust','wheatloosesmut']
        img_arr=img_arr/255
        img_arr = np.expand_dims(img_arr,axis=0)
        val = model.predict(img_arr)
        print(val)
        if(val[0][0]>val[0][1] and val[0][0]>val[0][2] and val[0][0]>val[0][3]):
          df.loc[df['path']=='data/test/'+str(img),label]='crownandrootrot'
        elif(val[0][1]>val[0][0] and val[0][1]>val[0][2] and val[0][1]>val[0][3]):
          df.loc[df['path']=='data/test/'+str(img),label]='healthywheat'
        elif(val[0][2]>val[0][0] and val[0][2]>val[0][1] and val[0][2]>val[0][3]):
          df.loc[df['path']=='data/test/'+str(img),label]='leafrust'
        elif(val[0][3]>val[0][0] and val[0][3]>val[0][2] and val[0][3]>val[0][1]):
          df.loc[df['path']=='data/test/'+str(img),label]='wheatloosesmut'
        count=count+1
        print(count)
        print(img_arr.shape)
    except:
        df.loc[df['path']=='data/test/'+str(img),label]='crownandrootrot'
        c=c+1
        print(c)
        continue
    # img_arr = cv2.resize(img_arr,(IMG_SIZE,IMG_SIZE))
    # img_arr=img_arr/255
    # img_arr = np.expand_dims(img_arr,axis=0)
    # val = model.predict(img_arr)
    # c=c+1
    # print(c)
    # print(val)
    # if(val[0][0]>val[0][1]):
    #   df.loc[df['path']=='data/test/' + str(img),label]='fake'
    # else:
    #   df.loc[df['path']=='data/test/' + str(img),label]='real'

df.to_csv('result2.csv', index=False)